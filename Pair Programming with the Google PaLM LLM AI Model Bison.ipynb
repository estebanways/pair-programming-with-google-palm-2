{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "<img align=\"right\" alt=\"PaLM 2\" src=\"https://i.imgur.com/KAEWX6i.jpg?raw=true\" width=\"1092\" height=\"614\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Pair Programming with the Google PaLM LLM AI Model Bison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Get the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API key is valid\n"
     ]
    }
   ],
   "source": [
    "# Check the Python environment packages\n",
    "#!conda list\n",
    "#!pip list\n",
    "\n",
    "# Install the Python packages\n",
    "#!conda install python-dotenv\n",
    "#!pip install python-dotenv\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "#from utils import get_api_key\n",
    "\n",
    "# Get the API key\n",
    "def get_api_key():\n",
    "    \n",
    "    _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "    return os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Test printing the API key\n",
    "if get_api_key():\n",
    "    print(\"The API key is valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Connect to the Google PaLM LLM AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: models/chat-bison-001\n",
      "description: Chat-optimized generative language model.\n",
      "generation methods:['generateMessage', 'countMessageTokens']\n",
      "\n",
      "name: models/text-bison-001\n",
      "description: Model targeted for text generation.\n",
      "generation methods:['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "\n",
      "name: models/embedding-gecko-001\n",
      "description: Obtain a distributed representation of a text.\n",
      "generation methods:['embedText']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the Google PaLM relevant libraries\n",
    "#!pip install -q google.generativeai\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import google.generativeai as palm\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "# Connect to the Google PaLM LLM AI Model\n",
    "palm.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "for m in palm.list_models():\n",
    "    print(f\"name: {m.name}\")\n",
    "    print(f\"description: {m.description}\")\n",
    "    print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Filter models by their supported generation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the text generation models from the list\n",
    "#models = [m for m in palm.list_models() \n",
    "#          if 'generateText' \n",
    "#          in m.supported_generation_methods]\n",
    "#models\n",
    "\n",
    "#model_bison = models[0]\n",
    "#model_bison\n",
    "\n",
    "# Filter the text generation models from the list\n",
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model_bison = models[0]\n",
    "#model_bison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Helper function to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google.api_core import retry\n",
    "\n",
    "# define a retry decorator\n",
    "@retry.Retry()\n",
    "\n",
    "# Helper function to generate text\n",
    "def generate_text(prompt,\n",
    "                  model=model_bison,\n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (494728963.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[122], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    **Prompts list**:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**Prompts list**:\n",
    "- Prompt using a simple prompt\n",
    "- Prompt using a template\n",
    "  - Pair Programming Scenarios\n",
    "    - Improve existing code\n",
    "    - Simplify code\n",
    "    - Write test cases\n",
    "    - Make code more efficient\n",
    "    - Debug your code\n",
    "  - Technical Debt\n",
    "    - Ask an LLM to explain a complex code base (or your own code!)\n",
    "    - Ask an LLM to document a complex code base (or your own code!)\n",
    "\n",
    "## Prompt using a simple prompt\n",
    "\n",
    "```python\n",
    "prompt = \"Show me how to iterate across a list in Python.\"\n",
    "#prompt = \"How updated are you as a model?\"  # 2023-03-08\n",
    "#prompt = \"Can you connect you to the internet\"  # yes\n",
    "#prompt = \"What is the URL of teh channel lescifi on YouTube?\"  # Wrong answer!\n",
    "#prompt = \"How is the weather in NY?\"\n",
    "#prompt = \"Where do you read the wether from?\"  # weather.com\n",
    "#prompt = \"What day and time is today?\"  # 2023-03-08 10:00:00\n",
    "\n",
    "# Generate text\n",
    "completion = generate_text(prompt)\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Prompt using a template\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "{priming}\n",
    "\n",
    "{question}\n",
    "\n",
    "{decorator}\n",
    "\n",
    "Your solution:\n",
    "\"\"\"\n",
    "\n",
    "priming_text = \"You are an expert at writing clear, concise, Python code.\"\n",
    "question = \"create a doubly linked list\"\n",
    "#decorator = \"Work through it step by step, and show your work. One step per line.\"\n",
    "decorator = \"Insert comments for each line of code.\"\n",
    "\n",
    "prompt = prompt_template.format(priming=priming_text,\n",
    "                                question=question,\n",
    "                                decorator=decorator)\n",
    "\n",
    "# Generate text\n",
    "completion = generate_text(prompt)\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Pair Programming Scenarios\n",
    "\n",
    "## Improve existing code\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explain, in detail, what you did to improve it.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "def func_x(array)\n",
    "  for i in range(len(array)):\n",
    "    print(array[i])\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Simplify code\n",
    "\n",
    "```python\n",
    "# option 1\n",
    "prompt_template = \"\"\"\n",
    "Can you please simplify this code for a linked list in Python?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you did to modify it, and why.\n",
    "\"\"\"\n",
    "\n",
    "# option 2\n",
    "prompt_template = \"\"\"\n",
    "Can you please simplify this code for a linked list in Python? \\n\n",
    "You are an expert in Pythonic code.\n",
    "\n",
    "{question}\n",
    "\n",
    "Please comment each line in detail, \\n\n",
    "and explain in detail what you did to modify it, and why.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.headval = None\n",
    "\n",
    "list1 = SLinkedList()\n",
    "list1.headval = Node(\"Mon\")\n",
    "e2 = Node(\"Tue\")\n",
    "e3 = Node(\"Wed\")\n",
    "list1.headval.nextval = e2\n",
    "e2.nextval = e3\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Write test cases\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "Can you please create test cases in code for this Python code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what these test cases are designed to achieve.\n",
    "\"\"\"\n",
    "\n",
    "# Note that the code I'm using here was output in the previous\n",
    "# section. Your output code may be different.\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.head = None\n",
    "\n",
    "def create_linked_list(data):\n",
    "  head = Node(data[0])\n",
    "  for i in range(1, len(data)):\n",
    "    node = Node(data[i])\n",
    "    node.nextval = head\n",
    "    head = node\n",
    "  return head\n",
    "\n",
    "list1 = create_linked_list([\"Mon\", \"Tue\", \"Wed\"])\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Make code more efficient\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "Can you please make this code more efficient?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you changed and why.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "# Returns index of x in arr if present, else -1\n",
    "def binary_search(arr, low, high, x):\n",
    "    # Check base case\n",
    "    if high >= low:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid] == x:\n",
    "            return mid\n",
    "        elif arr[mid] > x:\n",
    "            return binary_search(arr, low, mid - 1, x)\n",
    "        else:\n",
    "            return binary_search(arr, mid + 1, high, x)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Test array\n",
    "arr = [ 2, 3, 4, 10, 40 ]\n",
    "x = 10\n",
    "\n",
    "# Function call\n",
    "result = binary_search(arr, 0, len(arr)-1, x)\n",
    "\n",
    "if result != -1:\n",
    "    print(\"Element is present at index\", str(result))\n",
    "else:\n",
    "    print(\"Element is not present in array\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Debug your code\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"\n",
    "Can you please help me to debug this code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you found and why it was a bug.\n",
    "\"\"\"\n",
    "\n",
    "# I deliberately introduced a bug into this code! Let's see if the LLM can find it.\n",
    "# Note -- the model can't see this comment -- but the bug is in the\n",
    "# print function. There's a circumstance where nodes can be null, and trying\n",
    "# to print them would give a null error.\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "   def __init__(self, data):\n",
    "      self.data = data\n",
    "      self.next = None\n",
    "      self.prev = None\n",
    "\n",
    "class doubly_linked_list:\n",
    "   def __init__(self):\n",
    "      self.head = None\n",
    "\n",
    "# Adding data elements\n",
    "   def push(self, NewVal):\n",
    "      NewNode = Node(NewVal)\n",
    "      NewNode.next = self.head\n",
    "      if self.head is not None:\n",
    "         self.head.prev = NewNode\n",
    "      self.head = NewNode\n",
    "\n",
    "# Print the Doubly Linked list in order\n",
    "   def listprint(self, node):\n",
    "       print(node.data),\n",
    "       last = node\n",
    "       node = node.next\n",
    "\n",
    "dllist = doubly_linked_list()\n",
    "dllist.push(12)\n",
    "dllist.push(8)\n",
    "dllist.push(62)\n",
    "dllist.listprint(dllist.head)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Temperature = 0.0 results in more deterministic output\n",
    "# Since a temperature > 0 encourages more randomness in the LLM output, you may\n",
    "# want to run this code a couple times to see what it outputs.\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question),\n",
    "    temperature = 0.7\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Technical Debt\n",
    "\n",
    "## Ask an LLM to explain a complex code base (or your own code!)\n",
    "\n",
    "```python\n",
    "#@title Complex Code Block\n",
    "# Note: Taken from https://github.com/lmoroney/odmlbook/blob/63c0825094b2f44efc5c4d3226425a51990e73d6/BookSource/Chapter08/ios/cats_vs_dogs/CatVsDogClassifierSample/ModelDataHandler/ModelDataHandler.swift\n",
    "CODE_BLOCK = \"\"\"\n",
    "// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "//\n",
    "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "// you may not use this file except in compliance with the License.\n",
    "// You may obtain a copy of the License at\n",
    "//\n",
    "//    http://www.apache.org/licenses/LICENSE-2.0\n",
    "//\n",
    "// Unless required by applicable law or agreed to in writing, software\n",
    "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "// See the License for the specific language governing permissions and\n",
    "// limitations under the License.\n",
    "\n",
    "import CoreImage\n",
    "import TensorFlowLite\n",
    "import UIKit\n",
    "\n",
    "\n",
    "/// An inference from invoking the `Interpreter`.\n",
    "struct Inference {\n",
    "  let confidence: Float\n",
    "  let label: String\n",
    "}\n",
    "\n",
    "/// Information about a model file or labels file.\n",
    "typealias FileInfo = (name: String, extension: String)\n",
    "\n",
    "/// Information about the MobileNet model.\n",
    "enum MobileNet {\n",
    "  static let modelInfo: FileInfo = (name: \"converted_model\", extension: \"tflite\")\n",
    "}\n",
    "\n",
    "# More large and complex code here ...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Can you please explain how this code works?\n",
    "\n",
    "{question}\n",
    "\n",
    "Use a lot of detail and make it as clear as possible.\n",
    "\"\"\"\n",
    "\n",
    "# Keeping the prompt simple, just to show the LLM the Complex Code Block\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n",
    "\n",
    "## Ask an LLM to document a complex code base (or your own code!)\n",
    "\n",
    "```python\n",
    "CODE_BLOCK = \"\"\" \n",
    "# replace this with your own code\n",
    "def foo(a):\n",
    "  b = a + 1\n",
    "  return 2*b\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Please write technical documentation for this code and \\n\n",
    "make it easy for a non Python developer to understand:\n",
    "\n",
    "{question}\n",
    "\n",
    "Output the results in markdown\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctgQNn9PLXoy",
    "tags": []
   },
   "source": [
    "## Create your prompt in the following cell\n",
    "\n",
    "* **Hide the cell Prompt Templates before running all the cells**\n",
    "* **Check and run the results after they are generated**\n",
    "* **Remember to import packages as needed to make a resultant code work**\n",
    "----------------------- Start your prompts here... ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How updated are you as a model?\"\n",
    "# Generate text\n",
    "completion = generate_text(prompt)\n",
    "print(completion.result)\n",
    "\n",
    "# Review the prompt\n",
    "#print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
